y3 <- cred_interval[2,][idx]
meanPost <- rowMeans(ests)[idx]
medianPost <- median(ests)[idx]
lenT <- fish$length[1:11]
tempT <- fish$temp[1:11]
idx <- order(fish$length[1:11])
x <- fish$length[idx]
PosteriorEstimate <- function(x,sam)
{
sample <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
X %*% sample$beta
}
ests <- sapply(1:1000, FUN = function(x) PosteriorEstimate(fish$length, mtemp))
res <- x %*% (result$betaSample)
cred_interval <- apply(res, MARGIN = 1, quantile, probs = c(0.05, 0.95)) # Credible Interval
y1 <- rowMeans(res)[idx]
y2 <- cred_interval[1,][idx]
y3 <- cred_interval[2,][idx]
plot(lenT, tempT, pch = 20  ,xlab = "Length",ylab = "Temperature")
lines(x, y1, col='red', type='l', lwd=1) # Posterior Mean
lines(x, y2, col='blue', type='l', lwd=1) #
lines(x, y3, col='blue', type='l', lwd=1)
legend(x = 0, y=25, c("90% Equal tail", "Posterior mean"), col=c("blue", "red"), lwd = 2)
load('fish.RData')
y <- fish[,1]
X <- as.matrix(cbind(fish[,-1], "age2"=fish[,3]^2, "temp2" = fish[,4]^2, "agetemp" = fish[,3] * fish[,4]))
nCov <- ncol(X)
X
mod <- BayesLinReg(y = y, X = X, mu_0 = rep(0,nCov), Omega_0 = 1/0.01 * diag(nCov), v_0 = 1, sigma2_0 = betas <- colMeans(mod$betaSample)
''
mod <- BayesLinReg(y = y, X = X, mu_0 = rep(0,nCov), Omega_0 = 1/0.01 * diag(nCov), v_0 = 1, sigma2_0= 10000)
mod <- BayesLinReg(y = y, X = X, mu_0 = rep(0,nCov), Omega_0 = 1/0.01 * diag(nCov), v_0 = 1, sigma2_0= 10000, nIter = 5000)
mod
betas <- colMeans(mod$betaSample)
beta
betas
mod <- BayesLinReg(y = y, X = X, mu_0 = rep(0,nCov), Omega_0 = 0.01 * diag(nCov), v_0 = 1, sigma2_0= 10000, nIter = 5000)
betas <- colMeans(mod$betaSample)
betas
credI <- apply(mod$betaSample, 2, quantile, probs = c(0.025, 0.975))
colnames(credI) <- colnames(X)
credI
# 4a
x = weibull
logPostWeibull <- function(param, x){
theta1 = param[1]
theta2 = param[2]
logPost =   sum(dweibull(x, shape = theta1, scale = theta2, log=TRUE)) +
- 2*log(theta1*theta2)
return(logPost)
}
initVal = c(1,1)
optRes <- optim(par = initVal, fn  = logPostWeibull, gr = NULL, x, method = c("L-BFGS-B"),
lower = c(0.0001,0.0001), upper = c(Inf,Inf), control = list(fnscale = -1), hessian = TRUE)
postMean <- optRes$par # This is the mean vector
postCov <- -solve(optRes$hessian) # This is posterior covariance matrix
print(postMean)
print(postCov)
load(file = 'weibull.RData')
#Calculated posterior distribution on paper
postLogDist = function(alphaBeta, x) {
alpha = alphaBeta[1]
beta = alphaBeta[2]
n = length(x)
#print("n is ")
#print(n)
print(alpha)
print(beta)
print(n)
fstTerm = log(alpha^(n-2)) - log(beta^(alpha*n + 2))
print(log(beta^(alpha*n + 2)))
#print(fstTerm)
sndTerm = sum(log(x^(alpha - 1)))
trdTerm = (-1/(beta^alpha))*sum(x^alpha)
return(fstTerm + sndTerm + trdTerm)
}
initVals = c(0.1,0.1)
OptimResults<-optim(initVals,
postLogDist,
gr=NULL,
weibull,
lower = c(0.1,0.1),
method=c("L-BFGS-B"),
control=list(fnscale=-1),
hessian=TRUE)
print("Variables are ")
alphaBeta = as.matrix(OptimResults$par)
rownames(alphaBeta) = c("alpha", "beta")
print(OptimResults$par)
print("Cov matrix is")
covMatrix = -solve(OptimResults$hessian)
print(covMatrix)
print("To be honest, I am a bit skeptical regarding the beta value, and I am thinking whether I have made an error. I calculated the posterior on paper, and I might have made an error there or when I switched to logistic. However, the fact that I changed to logistic should not change the outcome; it should still give the same beta and alpha as they are both strictly growing functions. ")
#Page 8 lecture 8. Metropolis algorithm implemented, with the possibility to insert ANY parameters.
Metropolis <- function(theta, cHat, nSamples, sigma, data) {
#print("n is ")
#print(length(data))
#Sample theta_p given theta_i-1 from the proposal distribution
#initialize vector of samples to be sampled
thetaSamples = matrix(rep(0, length(theta*nSamples)),
nrow = nSamples,
ncol = length(theta))
thetaSamples[1,] = theta
for (i in 2:nSamples) {
#sample from the proposal distribution
thetaP = MASS::mvrnorm(n=1, mu = thetaSamples[i-1,], Sigma = cHat*sigma)
f = postLogDist(alphaBeta = thetaP, x = data)
s = postLogDist(alphaBeta = thetaSamples[i-1,], x= data)
#Calculating the probability (more stable with logs)
sndFactor = exp(f - s)
acceptanceProbability = min(1, sndFactor)
#toss unbiased coin
acc = rbinom(1, size = 1, prob = acceptanceProbability)
if (acc == 1) {
thetaSamples[i,] = thetaP
} else {
thetaSamples[i,] = thetaSamples[i-1,]
}
}
}
burnIn = 500
samples = 2000
nDraws = burnIn + samples
startVals = c(1,1)
#I am a bit skeptical about my result in a), so I use another covMatrix
covMatrix = diag(c(0.01, 0.01))
cHat = c(0.1,4,100)
print("I have done some error in the code, which makes me produce NaNs. I am not sure where in the code I have done wrong, but I suspect my log posterior calculation is wrong in some way. ")
for (i in 1:length(cHat)) {
if(i == 1) {
results1 = Metropolis(theta = startVals, cHat[i], nSamples = nDraws, sigma = covMatrix, data = weibull)
} else if (i == 2) {
results2 = Metropolis(theta = startVals, cHat[i], nSamples = nDraws, sigma = covMatrix, data = weibull)
} else if (i == 3) {
results3 = Metropolis(theta = startVals, cHat[i], nSamples = nDraws, sigma = covMatrix, data = weibull)
} else {
results4 = Metropolis(theta = startVals, cHat[i], nSamples = nDraws, sigma = covMatrix, data = weibull)
}
}
cHat
results1
results2
results3
results4
rbinom(1, size = 1, prob = acceptanceProbability)
rbinom(1, size = 1, prob = 0.2)
# 4a
x = weibull
logPostWeibull <- function(param, x){
theta1 = param[1]
theta2 = param[2]
logPost =   sum(dweibull(x, shape = theta1, scale = theta2, log=TRUE)) +
- 2*log(theta1*theta2)
return(logPost)
}
initVal = c(1,1)
optRes <- optim(par = initVal, fn  = logPostWeibull, gr = NULL, x, method = c("L-BFGS-B"),
lower = c(0.0001,0.0001), upper = c(Inf,Inf), control = list(fnscale = -1), hessian = TRUE)
postMean <- optRes$par # This is the mean vector
postCov <- -solve(optRes$hessian) # This is posterior covariance matrix
print(postMean)
print(postCov)
Metropolis <- function(c,niter,warmup,initVal,Sigma,logPostFunc,...) {
theta <- initVal
thetamat <- matrix(0,length(theta),warmup+niter)
thetamat[,1] <- theta
accprobvec <- rep(0,warmup+niter)
for(i in 2:(warmup+niter)) {
thetaProp <- t(rmvnorm(1,theta,c*Sigma))
thetaProp[thetaProp <= 0] = 1e-6
accprob <- exp(logPostFunc(thetaProp,...) - logPostFunc(theta,...))
accprobvec[i] <- min(accprob,1)
if(runif(1) < accprob) {
theta <- thetaProp
}
thetamat[,i] <- theta
}
return(list(thetamat=thetamat,accprobvec=accprobvec))
}
c <- 0.1
niter <- 2000
warmup <- 500
initVal <- c(1,1)
# Sigma = postCov
Sigma = diag(c(.01,.1))
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
rowMeans(mp$thetamat)
apply(mp$thetamat,1,var)
c <- 0.1
niter <- 2000
warmup <- 500
initVal <- c(1,1)
# Sigma = postCov
Sigma = diag(c(.01,.1))
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
###################
#### c = 4  #####
###################
c <- 0.4
niter <- 2000
warmup <- 500
initVal <- c(1,1)
# Sigma = postCov
Sigma = diag(c(.01,.1))
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
# 4a
x = weibull
logPostWeibull <- function(param, x){
theta1 = param[1]
theta2 = param[2]
logPost =   sum(dweibull(x, shape = theta1, scale = theta2, log=TRUE)) +
- 2*log(theta1*theta2)
return(logPost)
}
initVal = c(1,1)
optRes <- optim(par = initVal, fn  = logPostWeibull, gr = NULL, x, method = c("L-BFGS-B"),
lower = c(0.0001,0.0001), upper = c(Inf,Inf), control = list(fnscale = -1), hessian = TRUE)
postMean <- optRes$par # This is the mean vector
postCov <- -solve(optRes$hessian) # This is posterior covariance matrix
print(postMean)
print(postCov)
Metropolis <- function(c,niter,warmup,initVal,Sigma,logPostFunc,...) {
theta <- initVal
thetamat <- matrix(0,length(theta),warmup+niter)
thetamat[,1] <- theta
accprobvec <- rep(0,warmup+niter)
for(i in 2:(warmup+niter)) {
thetaProp <- t(rmvnorm(1,theta,c*Sigma))
thetaProp[thetaProp <= 0] = 1e-6
accprob <- exp(logPostFunc(thetaProp,...) - logPostFunc(theta,...))
accprobvec[i] <- min(accprob,1)
if(runif(1) < accprob) {
theta <- thetaProp
}
thetamat[,i] <- theta
}
return(list(thetamat=thetamat,accprobvec=accprobvec))
}
###################
#### c = 0.1  #####
###################
c <- 0.1
niter <- 2000
warmup <- 500
initVal <- c(1,1)
Sigma = postCov
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
###################
#### c = 4  #####
###################
c <- 0.4
niter <- 2000
warmup <- 500
initVal <- c(1,1)
Sigma = postCov
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
postCov
mean(mp$accprobvec)
###################
#### c = 4  #####
###################
c <- 4
niter <- 2000
warmup <- 500
initVal <- c(1,1)
Sigma = postCov
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
rowMeans(mp$thetamat)
apply(mp$thetamat,1,var)
###################
#### c = 100  #####
###################
c <- 100
niter <- 2000
warmup <- 500
initVal <- c(1,1)
Sigma = postCov
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
rowMeans(mp$thetamat)
apply(mp$thetamat,1,var)
###################
#### c = 4  #####
###################
c <- 4
niter <- 2000
warmup <- 500
initVal <- c(1,1)
Sigma = postCov
#Sigma = diag(c(.01,.1))
mp <- Metropolis(c,niter,warmup,initVal,Sigma,logPostWeibull,x)
n <- length(initVal)
theta_mean <- rowMeans(mp$thetamat[,(warmup+1):(warmup+niter)])
theta_var<- rep(0,n)
for(i in 1:n) {
theta_var[i] <- var(mp$thetamat[i,(warmup+1):(warmup+niter)])
}
par(mfrow=c(2,1))
plot(mp$thetamat[1,], type="l")
plot(mp$thetamat[2,], type="l")
mean(mp$accprobvec)
# We see good mixing in the chain and acceptance probabilities between 25-30%
# which is optimal according to the Lecture 8 slides.
rowMeans(mp$thetamat)
apply(mp$thetamat,1,var)
targetdensity <- function(theta, prior_mu, prior_sigma, X, Y, ...) {
likelihood <- dweibull(Y,shape = aplha, log = TRUE)
prior <- dmvnorm(theta, mean = prior_mu, sigma = prior_sigma, log = TRUE)
sum(likelihood) + prior
}
proposaldensity <- function(theta, mu, prop_sigma, ...){
dmvnorm(theta, mean = mu, sigma = prop_sigma, log = TRUE)
}
proposalsampler <- function(mu, prop_sigma, ...){
matrix(rmvnorm(1, mean = mu, sigma = prop_sigma), nrow = 1)
}
metropolis_hastings <- function(log_targ_post_func, log_prop_func, prop_sampler,
X0, iters, ...){
x <- X0
values <- matrix(0, ncol = length(X0), nrow = iters + 1)
values[1,] <- X0
alpha <- function(x, y, ...) {
numerator <- log_targ_post_func(y, ...) + log_prop_func(x, y, ...)
denominator <- log_targ_post_func(x, ...) + log_prop_func(y, x, ...)
exp(numerator - denominator)
}
for (i in 1:iters) {
y <- prop_sampler(x, ...)
u <- runif(1)
if (u < alpha(x, y, ...)) {
x <- y
}
values[i+1,] <- x
}
}
values
iters <- 10000
X0 <- rep(0, times = ncol(X))
params <- list(
log_targ_post_func = targetdensity,
log_prop_func = proposaldensity,
prop_sampler = proposalsampler,
X0 = matrix(rep(0, times = ncol(X)), nrow = 1),
iters = iters,
X = t(X),
Y = Y,
prior_mu = rep(0, times = ncol(X)),
prior_sigma = 100 * solve(t(X) %*% X),
prop_sigma = 0.6 * -solve(hessian)
)
weibull
library("mvtnorm")
n = ncol(data)
tau <- 10
covNames <- names(data)[2:n]
# after droping a column Work
nPara <- dim(X)[2]
prior_mu <- as.vector(rep(0,nPara))
I = diag(nPara) #identity matrix
prior_sigma = tau^2 * I
PostWeinbull <- function(X,a,b){
# evaluating the likelihood
Lik <-  ((a/b^a)^n) * sum((x)^(a-1)) *(exp(sum(-x)/b)^a)
if (abs(Lik) == Inf) logLik = -20000; # Likelihood is
#not finite, stear the optimizer away from here!
# evaluating the prior
Prior <- (1/a*b)^2
}
# add the log prior and log-likelihood together to get log posterior
return(Lik * Prior)
initial_value <- as.vector(rnorm(dim(X)[2]))
optim_r <- optim(initial_value,PostWeinbull,
gr=NULL,data,1,1,
method=c("BFGS"),
control=list(fnscale=-1),hessian=TRUE)
#numerical values of beta's_tilta
postMode <- optim_r$par
# Posterior covariance matrix is j^-1(b_tilta) -inv(Hessian)
j_inverse <- -solve(optim_r$hessian)
?dweibull
n = 50
prior <- function(alpha,beta){
xx <- log(1/(alpha*beta))^2
return(xx)
}
likelihood <- function(x,alpha,beta){
dweibull(x, alpha, beta, log = TRUE)
}
posterior <- function(betas,x){
likelihood(x,betas[1],betas[2]) + prior(betas[1],betas[2])
}
betas <- c(0,0)
OptimResults <- optim(par = betas,fn = posterior, method=c("L-BFGS-B") , hessian = TRUE,lower =0,
x = weibull)
theta <- c(0,0)
theta <- t(theta)
targetdensity <- function(theta, palpha,beta, ...) {
likelihood <- dweibull(x, alpha,betas, log = TRUE)
prior <- logprior(alpha,beta)
sum(likelihood) + prior
}
proposaldensity <- function(theta, mu, prop_sigma, ...){
dmvnorm(theta, mean = mu, sigma = prop_sigma, log = TRUE)
}
proposalsampler <- function(mu, prop_sigma, ...){
matrix(rmvnorm(1, mean = mu, sigma = prop_sigma), nrow = 1)
}
metropolis_hastings <- function(log_targ_post_func, log_prop_func, prop_sampler,
X0, iters, ...){
x <- X0
values <- matrix(0, ncol = length(X0), nrow = iters + 1)
values[1,] <- X0
alpha <- function(x, y, ...) {
numerator <- log_targ_post_func(y, ...) + log_prop_func(x, y, ...)
denominator <- log_targ_post_func(x, ...) + log_prop_func(y, x, ...)
exp(numerator - denominator)
}
for (i in 1:iters) {
y <- prop_sampler(x, ...)
u <- runif(1)
if (u < alpha(x, y, ...)) {
x <- y
}
values[i+1,] <- x
}
values
}
iters <- 10000
X0 <- rep(0, times = ncol(X))
params <- list(
log_targ_post_func = targetdensity,
log_prop_func = proposaldensity,
prop_sampler = proposalsampler,
X0 = matrix(rep(0, times = ncol(X)), nrow = 1),
iters = iters,
X = t(X),
Y = Y,
prior_mu = rep(0, times = ncol(X)),
prior_sigma = 100 * solve(t(X) %*% X),
prop_sigma = 0.6 * -solve(hessian)
)
metro_res <- do.call(metropolis_hastings, params)
11.01/(2.05/0.05)

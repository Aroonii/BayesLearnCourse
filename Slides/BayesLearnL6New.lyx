#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}
\end_preamble
\options xcolor=svgnames
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Bayesian Learning
\end_layout

\end_inset

Bayesian Learning - Lecture 6
\end_layout

\begin_layout Author
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Mattias Villani
\end_layout

\end_inset

Mattias Villani
\end_layout

\begin_layout Institute

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\series bold
Statistics, LiU
\end_layout

\end_inset

Division of Statistics
\begin_inset Newline newline
\end_inset

Department of Computer and Information Science
\begin_inset Newline newline
\end_inset

LinkÃ¶ping University 
\end_layout

\begin_layout Date
\begin_inset space \thinspace{}
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lecture overview
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Flexible nonlinear regression and splines
\end_layout

\begin_layout Itemize
Smoothness/shrinkage priors
\end_layout

\begin_layout Itemize
Gaussian process regression
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Recall the linear regression model with a single covariate
\begin_inset Formula 
\[
y_{i}=\beta_{0}+\beta_{1}x_{i}+\varepsilon_{i}\text{, \ \ \ \ \ }\varepsilon_{i}\overset{iid}{\sim}N(0,\sigma^{2}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Polynomial regression:
\begin_inset Formula 
\[
y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+...+\beta_{k}x_{i}^{k}+\varepsilon_{i}.
\]

\end_inset

This can be written as a linear regression
\begin_inset Formula 
\[
y=X_{P}\beta+\varepsilon,
\]

\end_inset

where 
\begin_inset Formula 
\[
X_{P}=(1,x,x^{2},...,x^{k}).
\]

\end_inset


\end_layout

\begin_layout Itemize
The posterior of 
\begin_inset Formula $\beta$
\end_inset

 is obtained like any linear regression.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Splines
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A more local basis: 
\series bold
truncated polynomials
\series default

\begin_inset Formula 
\[
b_{ij}=\left\{ \begin{array}{c}
(x_{i}-k_{j})^{p}\text{ \ \ if }x_{i}>k_{j}\\
0\text{ otherwise}
\end{array}\right.
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $k_{1},k_{2},...,k_{m}$
\end_inset

 are the 
\series bold
knots
\series default
.
\end_layout

\begin_layout Itemize
Splines are nonlinear in 
\begin_inset Formula $x$
\end_inset

, but linear in basis space 
\begin_inset Formula 
\[
y=X_{b}\beta+\varepsilon,
\]

\end_inset

where 
\begin_inset Formula $X_{b}$
\end_inset

 is the basis regression matrix 
\begin_inset Formula 
\[
X_{b}=(b_{1},...,b_{m}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Common extension
\begin_inset Formula 
\[
X_{b}=(1,x,b_{1},...,b_{m}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Still just linear in 
\begin_inset Formula $X_{b}$
\end_inset

.
 Linear regression fitting.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Smoothness prior for splines
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Problem: too many knots leads to 
\series bold
over-fitting
\series default
.
 
\end_layout

\begin_layout Itemize
Solution: 
\series bold
smoothness/shrinkage/regularization prior
\series default

\begin_inset Formula 
\[
\beta_{i}|\sigma^{2}\overset{iid}{\sim}N\left(0,\frac{\sigma^{2}}{\lambda}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Larger 
\begin_inset Formula $\lambda$
\end_inset

 gives smoother fit.
 Note: here we have 
\begin_inset Formula $\mbox{\Omega}_{0}=\lambda I$
\end_inset

.
\end_layout

\begin_layout Itemize
Equivalent to a penalized likelihood: 
\begin_inset Formula 
\[
-2\cdot LogPost\propto RSS(\beta)+\lambda\beta'\beta
\]

\end_inset


\end_layout

\begin_layout Itemize
Posterior mean gives 
\series bold
ridge regression
\series default
 estimator
\begin_inset Formula 
\[
\tilde{\beta}=\left(X'X+\lambda I\right)^{-1}X'y
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Shrinkage
\series default
 toward zero 
\begin_inset Formula 
\[
\text{As }\lambda\rightarrow\infty,\text{ }\tilde{\beta}\rightarrow0
\]

\end_inset


\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $X'X=I$
\end_inset

 
\begin_inset Formula 
\[
\tilde{\beta}=\frac{1}{1+\lambda}\hat{\beta}_{OLS}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian spline with smoothness prior
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /Users/matvi05/Dropbox/Teaching/BayesStat2011/Lectures/LidarPoly1Knots24.eps
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Smoothness prior for splines, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The famous 
\series bold
Lasso
\series default
 variable selection method is equivalent to using the posterior mode estimate
 under the prior:
\begin_inset Formula 
\[
\beta_{i}|\sigma^{2}\overset{iid}{\sim}\mathrm{Laplace}\left(0,\frac{\sigma^{2}}{\lambda}\right)
\]

\end_inset

where the general Laplace density is
\begin_inset Formula 
\[
p(\beta_{i})=\frac{\text{1}}{2b}\exp\left(-\frac{\left|\beta_{i}-\mu\right|}{b}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
The Bayesian shrinkage prior is 
\series bold
interpretable
\series default
, 
\series bold
not ad hoc
\series default
.
\end_layout

\begin_layout Itemize
Laplace distribution have heavy tails.
\end_layout

\begin_layout Itemize
Laplace prior: many 
\begin_inset Formula $\beta_{i}$
\end_inset

 close to zero, but some 
\begin_inset Formula $\beta_{i}$
\end_inset

 may be very large.
\end_layout

\begin_layout Itemize
Normal distribution have light tails.
 
\end_layout

\begin_layout Itemize
Normal prior: most 
\begin_inset Formula $\beta_{i}$
\end_inset

 are fairly equal in size, and no single 
\begin_inset Formula $\beta_{i}$
\end_inset

 can be very much larger than the other ones.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Estimating the shrinkage
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
How do we determine the degree of smoothness, 
\begin_inset Formula $\lambda$
\end_inset

? Cross-validation.
\end_layout

\begin_layout Itemize
Bayesian: I cannot specify 
\begin_inset Formula $\lambda$
\end_inset

 
\begin_inset Formula $\;\Rightarrow\;$
\end_inset


\begin_inset Formula $\lambda$
\end_inset

 is unknown
\begin_inset Formula $\;\Rightarrow\;$
\end_inset

use a prior for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Itemize
One possibility: 
\begin_inset Formula $\lambda\sim Inv-\chi^{2}(\eta_{0},\lambda_{0})$
\end_inset

.
 The user specifies 
\begin_inset Formula $\eta_{0}$
\end_inset

 and 
\begin_inset Formula $\lambda_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Alternative approach: specify the prior on the 
\emph on
degrees of freedom
\emph default
.
\end_layout

\begin_layout Itemize
Hierarchical setup:
\begin_inset Formula 
\begin{align*}
y|\beta,x & \sim N(x'\beta,\sigma^{2})\\
\beta|\sigma^{2} & \sim N\left(\left(\begin{array}{c}
0\\
0
\end{array}\right),\sigma^{2}\left(\begin{array}{cc}
\delta_{0}^{-1}I_{q} & 0\\
0 & \lambda^{-1}I_{m}
\end{array}\right)\right)\\
\sigma^{2} & \sim Inv-\chi^{2}(\nu_{0},\sigma_{0}^{2})\\
\lambda & \sim Inv-\chi^{2}(\eta_{0},\lambda_{0})
\end{align*}

\end_inset

Note: different shrinkage on the original 
\begin_inset Formula $q$
\end_inset

 covariates (
\begin_inset Formula $\delta_{0}$
\end_inset

) and the covariates that comes from the knots (
\begin_inset Formula $\lambda$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Estimating the shrinkage, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Joint posterior
\begin_inset Formula 
\[
p(\beta,\sigma^{2},\lambda|y,x)=p(\beta,\sigma^{2}|\lambda,y,x)p(\lambda|y,x)
\]

\end_inset

where 
\begin_inset Formula 
\[
p(\lambda|y,x)=\int\int p(\beta,\sigma,\lambda|y,x)d\beta d\sigma^{2}
\]

\end_inset

is the marginal posterior of 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
The conditional posterior 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p(\beta,\sigma^{2}|\lambda,y,x)$
\end_inset

 is a special case linear regression with conjugate prior 
\begin_inset Formula $\mu_{0}=(0,0)'$
\end_inset

 and
\begin_inset Formula 
\[
\Omega_{0}=\left(\begin{array}{cc}
\delta_{0}I_{q} & 0\\
0 & \lambda I_{m}
\end{array}\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Summary of the posterior with normal shrinkage prior
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The joint posterior of 
\begin_inset Formula $\beta$
\end_inset

, 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

 is
\begin_inset Formula 
\begin{align*}
\beta|\sigma^{2},\lambda,y & \sim N\left(\mu_{n},\Omega_{n}^{-1}\right)\\
\sigma^{2}|\lambda,y & \sim Inv-\chi^{2}\left(\nu_{n},\sigma_{n}^{2}\right)\\
p(\lambda|y) & \propto\sqrt{\frac{\left|\Omega_{0}\right|}{\left|X'X+\Omega_{0}\right|}}\left(\frac{\nu_{n}\sigma_{n}^{2}}{2}\right)^{-\nu_{n}/2}\cdot p(\lambda)
\end{align*}

\end_inset

where 
\begin_inset Formula $p(\lambda)$
\end_inset

 is the prior for 
\begin_inset Formula $\lambda$
\end_inset

, and 
\begin_inset Formula 
\begin{align*}
\mu_{n} & =\left(X'X+\Omega_{0}\right)^{-1}X'y\\
\Omega_{n} & =X'X+\Omega_{0}\\
\nu_{n} & =\nu_{0}+n\\
\nu_{n}\sigma_{n}^{2} & =\nu_{0}\sigma_{0}^{2}+y'y-\mu_{n}'\Omega_{n}\mu_{n}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian variable selection and estimating knot locations
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Selecting among a set of fixed knots 
\begin_inset Formula $k_{1},...,k_{m}$
\end_inset

 is a variable selection problem.
 More on 
\series bold
Bayesian variable selection
\series default
 in the last module.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
The 
\series bold
location of the knots
\series default
 can be treated as 
\series bold
unknown
\series default
, and estimated from the data.
 
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
The joint posterior of parameters and knot locations
\begin_inset Formula 
\[
p(\beta,\sigma^{2},\lambda,\xi_{1},...,\xi_{q}|y,x)
\]

\end_inset

where 
\begin_inset Formula $\xi_{i}$
\end_inset

 is the location of the 
\begin_inset Formula $i$
\end_inset

th knot.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
Posterior is complex but can be sampled from by Markov Chain Monte Carlo
 (MCMC).
 Li and Villani (2013, SJS).
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Non-parametric regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Linear regression
\series default

\begin_inset Formula 
\[
y=\beta\cdot x+\varepsilon
\]

\end_inset

where 
\begin_inset Formula $\varepsilon\sim N(0,\sigma^{2})$
\end_inset

 and iid over observations.
\end_layout

\begin_layout Itemize

\series bold
Nonlinear
\series default
 
\series bold
regression
\series default

\begin_inset Formula 
\[
y=f(x)+\varepsilon
\]

\end_inset

where 
\begin_inset Formula $f(\cdot)$
\end_inset

 is some nonlinear function (ex 
\begin_inset Formula $f(x)=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}$
\end_inset

).
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Non-parametric regression
\series default
: avoiding a parametric form for 
\begin_inset Formula $f(\cdot)$
\end_inset

.
\end_layout

\begin_layout Itemize
How do we put a 
\series bold
prior over a set of functions
\series default
?
\end_layout

\begin_layout Itemize
Restrict attention to a grid of (ordered) 
\begin_inset Formula $x$
\end_inset

-values: 
\begin_inset Formula $x_{1},x_{2},..,x_{k}$
\end_inset

.
\end_layout

\begin_layout Itemize
We can now put a joint prior on the 
\begin_inset Formula $k$
\end_inset

 function values: 
\begin_inset Formula $f(x_{1}),f(x_{2}),...,f(x_{k})$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Nonparametric = one parameter for every x!
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncSmoothManyPoints.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Nonparametric regression - smooth interpolation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Treat all 
\begin_inset Formula $n$
\end_inset

 ordinates as unknown parameters: 
\begin_inset Formula 
\[
f(x_{i})=\gamma_{i}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Problem: too many parameters.
 Estimated curve wiggles way too much.
\end_layout

\begin_layout Itemize
Solution: use a (multivariate) prior on 
\begin_inset Formula $\gamma=(\gamma_{1},...,\gamma_{n})^{\prime}$
\end_inset

 that carries the info that the regression curve is smooth:
\begin_inset Formula 
\[
\text{if }x_{i}\text{ and }x_{k}\text{ are close then }\gamma_{i}\text{ is }\text{close to }\gamma_{k}
\]

\end_inset


\end_layout

\begin_layout Itemize
Order the data with respect to covariates and assign the prior 
\begin_inset Formula 
\[
p(\gamma_{i}|\gamma_{i-1})\sim N\left(\gamma_{i-1},\tau_{0}^{2}\cdot\left|x_{i}-x_{i-1}\right|\right),\text{ for }i=2,...,n.
\]

\end_inset


\end_layout

\begin_layout Itemize
The hyperparameter 
\begin_inset Formula $\tau_{0}^{2}$
\end_inset

 controls the degree of prior smoothness.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian process regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Generalization of smooth interpolation.
 Multivariate normal (Gaussian) prior:
\begin_inset Formula 
\[
\left(\begin{array}{c}
f(x_{1})\\
\vdots\\
f(x_{k})
\end{array}\right)\sim N\left(\mathbf{m},\mathbf{K}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
But how do we specify the 
\begin_inset Formula $k\times k$
\end_inset

 
\series bold
covariance matrix
\series default
 
\begin_inset Formula $\mathbf{K}$
\end_inset

?
\begin_inset Formula 
\[
Cov\left(f(x_{p}),f(x_{q})\right)
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Squared exponential covariance function
\series default

\begin_inset Formula 
\[
Cov\left(f(x_{p}),f(x_{q})\right)=K(x_{p},x_{q})=\sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\frac{x_{p}-x_{q}}{\ell}\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
The covariance between 
\begin_inset Formula $f(x_{p})$
\end_inset

 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $f(x_{q})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is a function of 
\begin_inset Formula $x_{p}$
\end_inset

 and 
\begin_inset Formula $x_{q}$
\end_inset

.
\end_layout

\begin_layout Itemize
Nearby 
\begin_inset Formula $x$
\end_inset

's have highly correlated function ordinates 
\begin_inset Formula $f(x)$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
\end_layout

\begin_layout Itemize
We can compute 
\begin_inset Formula $Cov\left(f(x_{p}),f(x_{q})\right)$
\end_inset

 for 
\emph on
any
\emph default
 
\begin_inset Formula $x_{p}$
\end_inset

 and 
\begin_inset Formula $x_{q}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Smooth function - points nearby
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncSmoothTwoPointsNear.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Smooth function - points nearby
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncSmoothTwoPointsNearWithDensity.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Smooth function - points far apart
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncSmoothTwoPointsApart.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Smooth function - points far apart
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncSmoothTwoPointsApartWithDensity.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Jagged function - points nearby
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncJaggedTwoPointsNear.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Jagged function - points nearby
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncJaggedTwoPointsNearWithDensity.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Jagged function - points far apart
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncJaggedTwoPointsApart.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Jagged function - points far apart
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPFuncJaggedTwoPointsApartWithDensity.svg
	lyxscale 40
	scale 50
	clip

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian process regression, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
Gaussian process
\series default
 (
\series bold
GP
\series default
) is a collection of random variables, any finite number of which have a
 multivariate Gaussian distribution.
\end_layout

\begin_layout Itemize
A Gaussian process is really a 
\series bold
probability distribution over functions
\series default
 (curves).
 This is exactly what we want! No need for a grid!
\end_layout

\begin_layout Itemize
A GP is completely specified by a mean and a covariance function
\begin_inset Formula 
\[
m(x)=\mathrm{E}\left[f(x)\right]
\]

\end_inset


\begin_inset Formula 
\[
K(x,x')=E\left[\left(f(x)-m(x)\right)\left(f(x')-m(x')\right)\right]
\]

\end_inset

for any two inputs 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $x'$
\end_inset

 (note: this is 
\emph on
not
\emph default
 the transpose here).
\end_layout

\begin_layout Itemize
A Gaussian process (prior) is denoted by
\begin_inset Formula 
\[
f(x)\sim GP\left(m(x),K(x,x')\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian process regression, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Example:
\begin_inset Formula 
\begin{align*}
m(x) & =\sin(x)\\
K(x,x') & =\sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\frac{x_{p}-x_{q}}{\ell}\right)^{2}\right)
\end{align*}

\end_inset

where 
\begin_inset Formula $l>0$
\end_inset

 is the length scale.
 
\end_layout

\begin_layout Itemize
Larger 
\begin_inset Formula $l$
\end_inset

 gives more smoothness in 
\begin_inset Formula $f(x)$
\end_inset

.
\end_layout

\begin_layout Itemize
Simulate draw from 
\begin_inset Formula $f(x)\sim GP\left(m(x),K(x,x')\right)$
\end_inset

 over any grid 
\begin_inset Formula $x_{*}=(x_{1},...,x_{n})$
\end_inset

 by using that
\begin_inset Formula 
\[
f(x_{*})\sim N\left(m(x_{*}),K(x_{*},x_{*})\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simulating a GP - sine mean and SE kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPPriorDraws_l2_sigmaf1.svg
	scale 21

\end_inset


\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPPriorDraws_l1_sigmaf1.svg
	scale 21

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPPriorDraws_l05_sigmaf1.svg
	scale 21

\end_inset


\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPPriorDraws_l02_sigmaf1.svg
	scale 21

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPPriorDraws_l1_sigmaf02.svg
	scale 21

\end_inset


\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/GPPriorDraws_l2_sigmaf02.svg
	scale 21

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian process regression, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Model
\series default

\begin_inset Formula 
\[
y_{i}=f(\mathbf{\textnormal{x}}_{i})+\varepsilon_{i},\quad\varepsilon\overset{iid}{\sim}N(0,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Prior
\series default

\begin_inset Formula 
\[
f(x)\sim GP\left(0,K(x,x')\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
You have observed the data: 
\begin_inset Formula $\mathbf{x}=(x_{1},...,x_{n})$
\end_inset

' and 
\begin_inset Formula $\mathbf{y}=(y_{1},...,y_{n})'$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Goal: the posterior of 
\begin_inset Formula $f(\cdot)$
\end_inset

 over a grid of 
\begin_inset Formula $x$
\end_inset

-values: 
\begin_inset Formula $\mathbf{f}_{*}=\mathbf{f}(\mathbf{x}_{*})$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Intermediate step: joint distribution of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{f}_{*}$
\end_inset


\begin_inset Formula 
\[
\left(\begin{array}{c}
\mathbf{y}\\
\mathbf{f}_{*}
\end{array}\right)\sim N\left\{ \left(\begin{array}{c}
\mathbf{0}\\
\mathbf{0}
\end{array}\right),\left[\begin{array}{cc}
K(\mathbf{x},\mathbf{x})+\sigma^{2}I & K(\mathbf{x},\mathbf{x}_{*})\\
K(\mathbf{x}_{*},\mathbf{x}) & K(\mathbf{x}_{*},\mathbf{x}_{*})
\end{array}\right]\right\} 
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
posterior
\series default
 
\begin_inset Formula 
\[
\mathbf{f}_{*}|\mathbf{x},\mathbf{y},\mathbf{x}_{*}\sim N\left(\bar{\mathbf{f}}_{*},\mathrm{cov}(\mathbf{f}_{*})\right)
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\bar{\mathbf{f}}_{*} & =K(\mathbf{x}_{*},\mathbf{x})\left[K(\mathbf{x},\mathbf{x})+\sigma^{2}I\right]^{-1}\mathbf{y}\\
\mathrm{cov}(\mathbf{f}_{*}) & =K(\mathbf{x}_{*},\mathbf{x_{*}})-K(\mathbf{x}_{*},\mathbf{x})\left[K(\mathbf{x},\mathbf{x})+\sigma^{2}I\right]^{-1}K(\mathbf{x},\mathbf{x_{*}})
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning a noise-free Gaussian process
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll1_0.eps
	scale 32

\end_inset


\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll02_0.eps
	scale 32

\end_inset


\end_layout

\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning a noise-free Gaussian process
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll1_1.eps
	scale 32

\end_inset


\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll02_1.eps
	scale 32

\end_inset


\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning a noise-free Gaussian process
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll1_2.eps
	scale 32

\end_inset


\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll02_2.eps
	scale 32

\end_inset


\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning a noise-free Gaussian process
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll1_3.eps
	scale 32

\end_inset


\begin_inset Graphics
	filename ../../../Seminars/Vienna2014/ConditionalSimEll02_3.eps
	scale 32

\end_inset


\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example - Canadian wages
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/CanadianWages.eps
	scale 45

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior of f - 
\begin_inset Formula $\ell=0.2,0.5,1,2$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/CanadianIntervalsF.eps
	scale 25

\end_inset


\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/CanadianIntervalsF_ell05.eps
	scale 25

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/CanadianIntervalsF_ell1.eps
	scale 25

\end_inset


\begin_inset Graphics
	filename ../../AdvBayesLearn/VT2014/GaussianProcesses/Graphics/CanadianIntervalsF_ell2.eps
	scale 25

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_body
\end_document
